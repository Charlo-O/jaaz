"""
TransNetV2 ËßÜÈ¢ëÂú∫ÊôØÂàÜÂâ≤ÊúçÂä°
Áî®‰∫éÊ£ÄÊµãËßÜÈ¢ë‰∏≠ÁöÑÈïúÂ§¥ÂàáÊç¢ÁÇπÂπ∂ÊèêÂèñÂÖ≥ÈîÆÂ∏ß
"""

import os
import sys
import torch
import numpy as np
from PIL import Image
from io import BytesIO
import cv2
from typing import List, Tuple, Optional
from pathlib import Path

# Ê∑ªÂä† TransNetV2 Âà∞ Python Ë∑ØÂæÑ
TRANSNET_PATH = Path(__file__).parent.parent.parent / "TransNetV2" / "inference-pytorch"
sys.path.insert(0, str(TRANSNET_PATH))

from transnetv2_pytorch import TransNetV2

from services.config_service import FILES_DIR
from tools.utils.image_canvas_utils import generate_file_id


class TransNetService:
    """TransNetV2 ËßÜÈ¢ëÂú∫ÊôØÂàÜÂâ≤ÊúçÂä°"""
    
    def __init__(self):
        self.model: Optional[TransNetV2] = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self._initialized = False
    
    def _ensure_initialized(self):
        """Á°Æ‰øùÊ®°ÂûãÂ∑≤Âä†ËΩΩ"""
        if self._initialized:
            return
        
        print(f"üé¨ Loading TransNetV2 model on {self.device}...")
        self.model = TransNetV2()
        
        # Âä†ËΩΩÊùÉÈáç
        weights_path = TRANSNET_PATH / "transnetv2-pytorch-weights.pth"
        if not weights_path.exists():
            # Â∞ùËØï‰ªéÊñá‰ª∂Â§πÂä†ËΩΩ
            weights_dir = TRANSNET_PATH / "transnetv2-pytorch-weights"
            if weights_dir.exists():
                # ÈúÄË¶ÅÂêàÂπ∂ÊùÉÈáçÊñá‰ª∂
                weights_path = self._merge_weights(weights_dir)
            else:
                raise FileNotFoundError(
                    f"TransNetV2 weights not found at {weights_path}. "
                    "Please run convert_weights.py first."
                )
        
        state_dict = torch.load(weights_path, map_location=self.device)
        self.model.load_state_dict(state_dict)
        self.model.eval()
        
        if self.device == "cuda":
            self.model = self.model.cuda()
        
        self._initialized = True
        print("‚úÖ TransNetV2 model loaded successfully")
    
    def _merge_weights(self, weights_dir: Path) -> Path:
        """ÂêàÂπ∂ÂàÜÁâáÁöÑÊùÉÈáçÊñá‰ª∂"""
        import glob
        
        output_path = TRANSNET_PATH / "transnetv2-pytorch-weights.pth"
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÂàÜÁâáÊñá‰ª∂
        parts = sorted(glob.glob(str(weights_dir / "*.pth.*")))
        if parts:
            print(f"üîß Merging {len(parts)} weight file parts...")
            with open(output_path, 'wb') as outfile:
                for part in parts:
                    with open(part, 'rb') as infile:
                        outfile.write(infile.read())
            print("‚úÖ Weights merged successfully")
        else:
            # ÂèØËÉΩÁõ¥Êé•Êúâ .pth Êñá‰ª∂
            pth_files = list(weights_dir.glob("*.pth"))
            if pth_files:
                return pth_files[0]
            raise FileNotFoundError(f"No weight files found in {weights_dir}")
        
        return output_path
    
    def _extract_frames(self, video_path: str, target_size: Tuple[int, int] = (48, 27)) -> Tuple[np.ndarray, List[np.ndarray], float]:
        """
        ‰ªéËßÜÈ¢ë‰∏≠ÊèêÂèñÂ∏ß
        
        Args:
            video_path: ËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑ
            target_size: ÁõÆÊ†áÂ∞∫ÂØ∏ (width, height)ÔºåTransNetV2 ÈúÄË¶Å 48x27
            
        Returns:
            (Áî®‰∫éÊ®°ÂûãÁöÑÂ∏ßÊï∞ÁªÑ, ÂéüÂßãÂ∏ßÂàóË°®, fps)
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frames_for_model = []
        original_frames = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ‰øùÂ≠òÂéüÂßãÂ∏ßÔºàBGR -> RGBÔºâ
            original_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            
            # ‰∏∫Ê®°ÂûãË∞ÉÊï¥Â§ßÂ∞è
            resized = cv2.resize(frame, target_size)
            # BGR -> RGB
            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            frames_for_model.append(resized)
        
        cap.release()
        
        # ËΩ¨Êç¢‰∏∫ numpy Êï∞ÁªÑ
        frames_array = np.array(frames_for_model, dtype=np.uint8)
        
        return frames_array, original_frames, fps
    
    def detect_scenes(self, video_path: str, threshold: float = 0.5) -> List[int]:
        """
        Ê£ÄÊµãËßÜÈ¢ë‰∏≠ÁöÑÂú∫ÊôØÂàáÊç¢ÁÇπ
        
        Args:
            video_path: ËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑ
            threshold: Âú∫ÊôØÂàáÊç¢Ê£ÄÊµãÈòàÂÄº
            
        Returns:
            Âú∫ÊôØÂàáÊç¢Â∏ßÁöÑÁ¥¢ÂºïÂàóË°®
        """
        self._ensure_initialized()
        
        frames_array, _, fps = self._extract_frames(video_path)
        
        if len(frames_array) == 0:
            return []
        
        print(f"üé¨ Processing {len(frames_array)} frames...")
        
        # ÂàÜÊâπÂ§ÑÁêÜÔºàÈÅøÂÖçÂÜÖÂ≠òÊ∫¢Âá∫Ôºâ
        batch_size = 100
        all_predictions = []
        
        with torch.no_grad():
            for i in range(0, len(frames_array), batch_size - 10):  # ÈáçÂè†10Â∏ß
                end_idx = min(i + batch_size, len(frames_array))
                batch = frames_array[i:end_idx]
                
                # Ê∑ªÂä† batch Áª¥Â∫¶
                batch_tensor = torch.from_numpy(batch).unsqueeze(0)
                
                if self.device == "cuda":
                    batch_tensor = batch_tensor.cuda()
                
                # Ê®°ÂûãÊé®ÁêÜ
                single_frame_pred, _ = self.model(batch_tensor)
                predictions = torch.sigmoid(single_frame_pred).cpu().numpy()[0, :, 0]
                
                if i == 0:
                    all_predictions.extend(predictions.tolist())
                else:
                    # Ë∑≥ËøáÈáçÂè†ÈÉ®ÂàÜ
                    all_predictions.extend(predictions[10:].tolist())
        
        # ÊâæÂà∞Âú∫ÊôØÂàáÊç¢ÁÇπ
        scene_changes = []
        for i, pred in enumerate(all_predictions):
            if pred > threshold:
                scene_changes.append(i)
        
        print(f"‚úÖ Detected {len(scene_changes)} scene changes")
        return scene_changes
    
    def extract_keyframes(
        self, 
        video_path: str, 
        threshold: float = 0.5,
        min_scene_length: int = 10
    ) -> List[dict]:
        """
        ÊèêÂèñËßÜÈ¢ëÂÖ≥ÈîÆÂ∏ßÔºàÊØè‰∏™Âú∫ÊôØÁöÑÁ¨¨‰∏ÄÂ∏ßÔºâ
        
        Args:
            video_path: ËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑ
            threshold: Âú∫ÊôØÂàáÊç¢Ê£ÄÊµãÈòàÂÄº
            min_scene_length: ÊúÄÂ∞èÂú∫ÊôØÈïøÂ∫¶ÔºàÂ∏ßÊï∞Ôºâ
            
        Returns:
            ÂÖ≥ÈîÆÂ∏ß‰ø°ÊÅØÂàóË°®ÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´ {file_id, url, width, height, frame_index, timestamp}
        """
        self._ensure_initialized()
        
        frames_array, original_frames, fps = self._extract_frames(video_path)
        
        if len(frames_array) == 0:
            return []
        
        print(f"üé¨ Processing {len(frames_array)} frames for keyframe extraction...")
        
        # Ê£ÄÊµãÂú∫ÊôØÂàáÊç¢ÁÇπ
        scene_changes = self.detect_scenes(video_path, threshold)
        
        # Ê∑ªÂä†Á¨¨‰∏ÄÂ∏ß‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™Âú∫ÊôØÁöÑÂºÄÂßã
        keyframe_indices = [0]
        
        # ËøáÊª§Â§™Áü≠ÁöÑÂú∫ÊôØ
        for i, change_idx in enumerate(scene_changes):
            if change_idx - keyframe_indices[-1] >= min_scene_length:
                keyframe_indices.append(change_idx)
        
        print(f"üì∏ Extracting {len(keyframe_indices)} keyframes...")
        
        # ‰øùÂ≠òÂÖ≥ÈîÆÂ∏ß
        keyframes = []
        for frame_idx in keyframe_indices:
            if frame_idx >= len(original_frames):
                continue
            
            frame = original_frames[frame_idx]
            height, width = frame.shape[:2]
            
            # ÁîüÊàêÊñá‰ª∂ ID Âπ∂‰øùÂ≠ò
            file_id = generate_file_id()
            filename = f"{file_id}.jpg"
            file_path = os.path.join(FILES_DIR, filename)
            
            # ‰øùÂ≠ò‰∏∫ JPEG
            img = Image.fromarray(frame)
            img.save(file_path, "JPEG", quality=95)
            
            # ËÆ°ÁÆóÊó∂Èó¥Êà≥
            timestamp = frame_idx / fps if fps > 0 else 0
            
            keyframes.append({
                "file_id": filename,
                "url": f"/api/file/{filename}",
                "width": width,
                "height": height,
                "frame_index": frame_idx,
                "timestamp": round(timestamp, 2),
            })
        
        print(f"‚úÖ Extracted {len(keyframes)} keyframes")
        return keyframes
    
    def extract_keyframes_simple(
        self, 
        video_path: str, 
        num_frames: int = 10
    ) -> List[dict]:
        """
        ÁÆÄÂçïÁöÑÂÖ≥ÈîÆÂ∏ßÊèêÂèñÔºàÂùáÂåÄÈááÊ†∑ÔºâÔºå‰∏ç‰ΩøÁî® TransNetV2
        Áî®‰∫éÂø´ÈÄüÊèêÂèñÊàñ‰Ωú‰∏∫Â§áÈÄâÊñπÊ°à
        
        Args:
            video_path: ËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑ
            num_frames: Ë¶ÅÊèêÂèñÁöÑÂ∏ßÊï∞
            
        Returns:
            ÂÖ≥ÈîÆÂ∏ß‰ø°ÊÅØÂàóË°®
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        if total_frames == 0:
            cap.release()
            return []
        
        # ËÆ°ÁÆóÈááÊ†∑Èó¥Èöî
        interval = max(1, total_frames // num_frames)
        frame_indices = list(range(0, total_frames, interval))[:num_frames]
        
        keyframes = []
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if not ret:
                continue
            
            # BGR -> RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            height, width = frame.shape[:2]
            
            # ÁîüÊàêÊñá‰ª∂ ID Âπ∂‰øùÂ≠ò
            file_id = generate_file_id()
            filename = f"{file_id}.jpg"
            file_path = os.path.join(FILES_DIR, filename)
            
            img = Image.fromarray(frame)
            img.save(file_path, "JPEG", quality=95)
            
            timestamp = frame_idx / fps if fps > 0 else 0
            
            keyframes.append({
                "file_id": filename,
                "url": f"/api/file/{filename}",
                "width": width,
                "height": height,
                "frame_index": frame_idx,
                "timestamp": round(timestamp, 2),
            })
        
        cap.release()
        print(f"‚úÖ Extracted {len(keyframes)} keyframes (simple mode)")
        return keyframes


# ÂÖ®Â±ÄÂçï‰æã
transnet_service = TransNetService()
